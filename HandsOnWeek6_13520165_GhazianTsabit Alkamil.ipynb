{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0uDup6H5Nakk"
   },
   "source": [
    "# HandsOn Week 6 RDD Spark\n",
    "Selamat datang di HandsOn Week 6, yaitu tentang pemrosesan data terdistribusi menggunakan Spark. Untuk tujuan pembelajaran, seperti biasa, kita akan menggunakan *pseudo-distributed mode* (single node cluster) di VM yang telah disediakan. Dengan kode yang *similar* di cluster komputer dengan *n* workers, maka komputasi akan tersebar ke *n* workers tersebut. Adapun yang akan kita coba kali ini adalah melakukan komputasi menggunakan RDD dan DataFrame. Berikut catatan-catatan yang perlu kamu perhatikan dalam hands-on ini:\n",
    "1. Untuk menjalankan Apache Spark dalam bahasa python di VM, ketikkan perintah ```pyspark``` di terminal.\n",
    "2. Dari semua Milestone, data input yang digunakan adalah data \"purchases.txt\" yang telah diletakkan di HDFS. Oleh karena itu, pastikan hadoop service kamu berjalan (```start-dfs.sh```, ```start-yarn.sh```, ```jps```). Untuk membaca data dari HDFS, lihat kembali di slide perkuliahan.\n",
    "3. Untuk Milestone 1, 2 dan 3, kalian perlu untuk mencatat waktu yang diperlukan saat melakukan MapReduce menggunakan hadoop streaming jar di hands-on sebelumnya. Waktu bisa dihitung dari selisih \"waktu awal\" dan \"waktu akhir\" yang tampak di terminal saat kalian selesain melakukan MapReduce -atau menggunakan cara lain yang masih *acceptable*-. (lihat ilustrasi di bawah).\n",
    "4. Lakukan zip file jupyter notebook ini beserta gambar-gambar yang diperlukan -screenshot waktu proses MapReduce Hadoop jar-, dan submit ke portal kuliah EDUNEX dengan format nama \"**HandsOnWeek10_NIM_NamaLengkap.zip**\". Pastikan file jupyter notebook yang kamu zip dalam kondisi memiliki output per cellnya (tidak kosong karena belum dijalankan). <br>\n",
    "\n",
    "<img title=\"Waktu Awal\" align=\"left\" src=\"waktu_awal.JPG\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "<img title=\"Waktu Akhir\" align=\"left\" src=\"waktu_akhir.JPG\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HJOcG2mNaks"
   },
   "source": [
    "## Milestone 1\n",
    "Kerjakan Milestone 1 pada HandsOn Week 5(sebelumnya), akan tetapi menggunakan RDD Spark. Catat waktu (bandingkan) yang dibutuhkan (dalam detik) antara: \"MapReduce menggunakan hadoop streaming jar\" dengan yang akan kamu proses menggunakan RDD Spark ini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "caB7L1cKNaks",
    "outputId": "5f26548d-0307-412c-939b-5ec799f6bab2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total sales:  [('Consumer Electronics', 57452374.12999931), ('Toys', 57463477.10999949)]\n",
      "Waktu yang diperlukan dengan RDD Spark: 18.92 detik\n",
      "Waktu yang diperlukan dengan Hadoop: 86 detik\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "# Tuliskan code kamu di sini\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "dist_data = sc.textFile(\"hdfs://localhost:9000/purchases/purchases.txt\")\n",
    "\n",
    "def mapper_milestone1(line):\n",
    "    line = line.strip()\n",
    "    datas = line.split('\\t')\n",
    "    if len(datas) == 6:\n",
    "        date, time, location, product, price, payment = datas\n",
    "        if 'Toys' in product or 'Consumer Electronics' in product:\n",
    "            return [(product, float(price))]\n",
    "    return []\n",
    "\n",
    "RDD1 = dist_data.flatMap(mapper_milestone1).filter(lambda x: x) \n",
    "total_sales = RDD1.reduceByKey(lambda x, y: x+y)\n",
    "print(\"total sales: \", total_sales.collect())\n",
    "\n",
    "print(\"Waktu yang diperlukan dengan RDD Spark: %.2f detik\" % (time() - start_time))\n",
    "print(\"Waktu yang diperlukan dengan Hadoop:\", \"86 detik\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4gk9-rwNakv"
   },
   "source": [
    "<img title=\"Waktu Awal\" align=\"left\" src=\"waktu_awal_milestone1.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "<img title=\"Waktu Akhir\" align=\"left\" src=\"waktu_akhir_milestone1.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aEb0MYjsNakv"
   },
   "source": [
    "## Milestone 2\n",
    "Kerjakan Milestone 2 pada HandsOn Week5 (sebelumnya), akan tetapi menggunakan RDD Spark. Catat waktu (bandingkan) yang dibutuhkan (dalam detik) antara: \"MapReduce menggunakan hadoop streaming jar\" dengan yang akan kamu proses menggunakan RDD Spark ini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "eQskYTrwNakw",
    "outputId": "05cc1653-2be4-4cf2-ef9e-db92793e75b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total sales by store:  [('San Francisco', (499.97, \"Men's Clothing\")), ('Atlanta', (499.96, 'Pet Supplies')), ('Miami', (499.98, \"Men's Clothing\"))]\n",
      "Waktu yang diperlukan dengan RDD Spark: 20.61 detik\n",
      "Waktu yang diperlukan dengan Hadoop: 59 detik\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "# Tuliskan code kamu di sini\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "dist_data = sc.textFile(\"hdfs://localhost:9000/purchases/purchases.txt\")\n",
    "\n",
    "def mapper_milestone2(line):\n",
    "    line = line.strip()\n",
    "    datas = line.split('\\t')\n",
    "    if len(datas) == 6:\n",
    "        date, time, location, product, price, payment = datas\n",
    "        if location == 'Miami' or location == 'San Francisco' or location == 'Atlanta':\n",
    "            return [(location, (float(price), product))]\n",
    "    return []\n",
    "\n",
    "RDD1 = dist_data.flatMap(mapper_milestone2).filter(lambda x: x)\n",
    "\n",
    "def reducer_milestone2(a, b):\n",
    "    if a[0] > b[0]:\n",
    "        return a\n",
    "    else:\n",
    "        return b\n",
    "    \n",
    "total_sales_by_store = RDD1.reduceByKey(reducer_milestone2)\n",
    "print(\"total sales by store: \", total_sales_by_store.collect())\n",
    "\n",
    "print(\"Waktu yang diperlukan dengan RDD Spark: %.2f detik\" % (time() - start_time))\n",
    "print(\"Waktu yang diperlukan dengan Hadoop:\", \"59 detik\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KeKZRzqCNakx"
   },
   "source": [
    "<img title=\"Waktu Awal\" align=\"left\" src=\"waktu_awal_milestone2.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "<img title=\"Waktu Akhir\" align=\"left\" src=\"waktu_akhir_milestone2.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aurKREgYNaky"
   },
   "source": [
    "## Milestone 3\n",
    "Kerjakan Milestone 3 pada HandsOn Week5 (sebelumnya), akan tetapi menggunakan RDD Spark. Catat waktu (bandingkan) yang dibutuhkan (dalam detik) antara: \"MapReduce menggunakan hadoop streaming jar\" dengan yang akan kamu proses menggunakan RDD Spark ini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "RoWcOJtYNakz",
    "outputId": "9be42dd8-b6b5-4f66-e415-82d468164690"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count sales by time:  [('09:01-10:00', 459775), ('10:01-11:00', 459825)]\n",
      "Waktu yang diperlukan dengan RDD Spark: 102.47 detik\n",
      "Waktu yang diperlukan dengan Hadoop: 103 detik\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, time\n",
    "# Tuliskan code kamu di sini\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "dist_data = sc.textFile(\"hdfs://localhost:9000/purchases/purchases.txt\")\n",
    "\n",
    "def mapper_milestone3(line):\n",
    "    line = line.strip()\n",
    "    datas = line.split('\\t')\n",
    "    if len(datas) == 6:\n",
    "        date, time_str, location, product, price, payment = datas\n",
    "        time_obj = datetime.strptime(time_str, '%H:%M').time()\n",
    "        if time(9,1) <= time_obj <= time(10,0):\n",
    "            return [('09:01-10:00',1)]\n",
    "        elif time(10, 1) <= time_obj <= time(11,0):\n",
    "            return [('10:01-11:00',1)] \n",
    "    return []\n",
    "\n",
    "RDD1 = dist_data.flatMap(mapper_milestone3).filter(lambda x: x)    \n",
    "count_sales_by_time = RDD1.reduceByKey(lambda x, y: x+y)\n",
    "print(\"count sales by time: \", count_sales_by_time.collect())\n",
    "\n",
    "print(\"Waktu yang diperlukan dengan RDD Spark: %.2f detik\" % ((datetime.now() - start_time).total_seconds()))\n",
    "print(\"Waktu yang diperlukan dengan Hadoop:\", \"103 detik\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYXjDgBzNak0"
   },
   "source": [
    "<img title=\"Waktu Awal\" align=\"left\" src=\"waktu_awal_milestone3.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "<img title=\"Waktu Akhir\" align=\"left\" src=\"waktu_akhir_milestone3.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aBWr5glANak1"
   },
   "source": [
    "## Milestone 4\n",
    "Milestone ini dibagi menjadi 4.1, 4.2 dan 4.3 yang masing-masing secara berturut-turut adalah mengerjakan ulang Milestone 1, 2 dan 3 di atas (menggunakan RDD Spark), akan tetapi menggunakan trik \"**persist() RDD**\" untuk mempercepat prosesnya. Kamu bisa melakukan \"**persist**\" untuk RDD mana saja yang kamu anggap dapat memberikan waktu proses tercepat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "ogD4MxwoNak1",
    "outputId": "450d6cb3-38ff-4021-9fc1-0778e5ac030c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total sales:  [('Consumer Electronics', 57452374.12999931), ('Toys', 57463477.10999949)]\n",
      "Waktu Milestone 1: 18.92 detik  vs. Waktu Milestone 4.1: 8.17 detik\n",
      "total sales by store:  [('San Francisco', (499.97, \"Men's Clothing\")), ('Atlanta', (499.96, 'Pet Supplies')), ('Miami', (499.98, \"Men's Clothing\"))]\n",
      "Waktu Milestone 2: 20.61 detik  vs. Waktu Milestone 4.2: 7.14 detik\n",
      "count sales by time:  [('09:01-10:00', 459775), ('10:01-11:00', 459825)]\n",
      "Waktu Milestone 3: 102.47 detik  vs. Waktu Milestone 4.3: 54.05 detik\n"
     ]
    }
   ],
   "source": [
    "from pyspark import StorageLevel\n",
    "from datetime import datetime, time\n",
    "\n",
    "dist_data = sc.textFile(\"hdfs://localhost:9000/purchases/purchases.txt\")\n",
    "\n",
    "start_time1 = datetime.now()\n",
    "\n",
    "filtered_data = dist_data.filter(lambda line: \"Toys\" in line or \"Consumer Electronics\" in line) \n",
    "filtered_data.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "def mapper_milestone1(line):\n",
    "    line = line.strip()\n",
    "    datas = line.split('\\t')\n",
    "    if len(datas) == 6:\n",
    "        date, time, location, product, price, payment = datas\n",
    "        return [(product, float(price))]\n",
    "    return []\n",
    "\n",
    "sales = filtered_data.flatMap(mapper_milestone1).filter(lambda x: x)\n",
    "\n",
    "total_sales = sales.reduceByKey(lambda x, y: x+y)\n",
    "print(\"total sales: \", total_sales.collect())\n",
    "\n",
    "end_time1 = datetime.now()\n",
    "\n",
    "print(\"Waktu Milestone 1:\", \"18.92 detik\", \" vs. Waktu Milestone 4.1: %.2f detik\" % ((end_time1 - start_time1).total_seconds()))\n",
    "\n",
    "start_time2 = datetime.now()\n",
    "\n",
    "filtered_data_2 = dist_data.filter(lambda line: \"Miami\" in line or \"San Francisco\" in line or 'Atlanta' in line) \n",
    "filtered_data_2.persist()\n",
    "\n",
    "def mapper_milestone2(line):\n",
    "    line = line.strip()\n",
    "    datas = line.split('\\t')\n",
    "    if len(datas) == 6:\n",
    "        date, time, location, product, price, payment = datas\n",
    "        return [(location, (float(price), product))]\n",
    "    return []\n",
    "\n",
    "RDD2 = filtered_data_2.flatMap(mapper_milestone2).filter(lambda x: x)\n",
    "\n",
    "def reducer_milestone2(a, b):\n",
    "    if a[0] > b[0]:\n",
    "        return a\n",
    "    else:\n",
    "        return b\n",
    "    \n",
    "total_sales_by_store = RDD2.reduceByKey(reducer_milestone2)\n",
    "print(\"total sales by store: \", total_sales_by_store.collect())\n",
    "\n",
    "end_time2 = datetime.now()\n",
    "\n",
    "print(\"Waktu Milestone 2:\", \"20.61 detik\", \" vs. Waktu Milestone 4.2: %.2f detik\" % ((end_time2 - start_time2).total_seconds()))\n",
    "\n",
    "start_time3 = datetime.now()\n",
    "\n",
    "def filter_milestone3(line):\n",
    "    line = line.strip()\n",
    "    datas = line.split('\\t')\n",
    "    if len(datas) == 6:\n",
    "        date, time_str, location, product, price, payment = datas\n",
    "        time_obj = datetime.strptime(time_str, '%H:%M').time()\n",
    "        if time(9,1) <= time_obj <= time(10,0):\n",
    "            return line\n",
    "        elif time(10, 1) <= time_obj <= time(11,0):\n",
    "            return line \n",
    "\n",
    "filtered_data3 = dist_data.filter(filter_milestone3)\n",
    "filtered_data3.persist()        \n",
    "\n",
    "def mapper_milestone3(line):\n",
    "    line = line.strip()\n",
    "    datas = line.split('\\t')\n",
    "    if len(datas) == 6:\n",
    "        date, time_str, location, product, price, payment = datas\n",
    "        time_obj = datetime.strptime(time_str, '%H:%M').time()\n",
    "        if time(9,1) <= time_obj <= time(10,0):\n",
    "            return [('09:01-10:00',1)]\n",
    "        elif time(10, 1) <= time_obj <= time(11,0):\n",
    "            return [('10:01-11:00',1)] \n",
    "    return []\n",
    "\n",
    "RDD3 = filtered_data3.flatMap(mapper_milestone3).filter(lambda x: x)\n",
    "count_sales_by_time = RDD3.reduceByKey(lambda x, y: x+y)\n",
    "print(\"count sales by time: \", count_sales_by_time.collect())\n",
    "\n",
    "end_time3 = datetime.now()\n",
    "\n",
    "print(\"Waktu Milestone 3:\", \"102.47 detik\", \" vs. Waktu Milestone 4.3: %.2f detik\" % ((end_time3 - start_time3).total_seconds()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-in09VQdNak2"
   },
   "source": [
    "## Milestone 5\n",
    "Milestone ini dibagi menjadi 5.1, 5.2 dan 5.3 yang masing-masing secara berturut-turut adalah mengerjakan ulang Milestone 1, 2 dan 3 di atas, akan tetapi menggunakan DataFrame dari Apache Spark. Catat waktu yang diperlukan untuk masing-masing proses (5.1, 5.2 dan 5.3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "KWw3UZWiNak2",
    "outputId": "3963d841-253b-405f-d8d9-5506dd4a0c61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|             product|        total_sales|\n",
      "+--------------------+-------------------+\n",
      "|Consumer Electronics|5.745237412999931E7|\n",
      "|                Toys|5.746347710999949E7|\n",
      "+--------------------+-------------------+\n",
      "\n",
      "+-------------+------+--------------+\n",
      "|     location| price|       product|\n",
      "+-------------+------+--------------+\n",
      "|San Francisco|499.97|Men's Clothing|\n",
      "|      Atlanta|499.96|  Pet Supplies|\n",
      "|        Miami|499.98|   Video Games|\n",
      "+-------------+------+--------------+\n",
      "\n",
      "+-----------+------+\n",
      "| Time Range| Count|\n",
      "+-----------+------+\n",
      "|09:01-10:00|459775|\n",
      "|10:01-11:00|459825|\n",
      "+-----------+------+\n",
      "\n",
      "Waktu Milestone 5.1: 15.87 detik\n",
      "Waktu Milestone 5.2: 18.43 detik\n",
      "Waktu Milestone 5.3: 41.95 detik\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import hour, minute, to_timestamp\n",
    "from time import time\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"dataframe-spark\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "purchases_df = spark.read.csv(\"hdfs://localhost:9000/purchases/purchases.txt\", inferSchema=True, sep=\"\\t\")\n",
    "\n",
    "purchases_df.createOrReplaceTempView(\"purchaseSql\")\n",
    "\n",
    "purchases_df = purchases_df.withColumn(\"_c1\", to_timestamp(\"_c1\", \"HH:mm\"))\n",
    "\n",
    "start_time1 = time()\n",
    "\n",
    "spark.sql(\"SELECT _c3 as product, SUM(_c4) as total_sales FROM purchaseSql WHERE _c3 LIKE '%Toys%' or _c3 LIKE '%Consumer Electronics%' GROUP BY _c3;\").show()\n",
    "\n",
    "end_time1 = time()\n",
    "\n",
    "start_time2 = time()\n",
    "\n",
    "spark.sql(\"SELECT location, price, product FROM (SELECT _c2 as location, _c4 as price, _c3 as product, ROW_NUMBER() OVER (PARTITION BY _c2 ORDER BY _c4 DESC) AS rank FROM purchaseSql WHERE _c2 IN ('Atlanta', 'Miami', 'San Francisco')) ranked WHERE rank = 1;\").show()\n",
    "\n",
    "end_time2 = time()\n",
    "\n",
    "start_time3 = time()\n",
    "\n",
    "spark.sql(\"\"\"SELECT \n",
    "    CONCAT('09:01-10:00') AS `Time Range`,\n",
    "    SUM(CASE WHEN (hour(_c1) = 9 AND minute(_c1) BETWEEN 1 AND 59 OR hour(_c1) = 10 AND minute(_c1) = 0) THEN 1 ELSE 0 END) AS `Count`\n",
    "FROM purchaseSql\n",
    "UNION ALL\n",
    "SELECT \n",
    "    CONCAT('10:01-11:00') AS `Time Range`,\n",
    "    SUM(CASE WHEN (hour(_c1) = 10 AND minute(_c1) BETWEEN 1 AND 59 OR hour(_c1) = 11 AND minute(_c1) = 0) THEN 1 ELSE 0 END) AS `Count`\n",
    "FROM purchaseSql;\"\"\").show()\n",
    "\n",
    "end_time3 = time()\n",
    "\n",
    "print(\"Waktu Milestone 5.1: %.2f detik\" % (end_time1 - start_time1))\n",
    "print(\"Waktu Milestone 5.2: %.2f detik\" % (end_time2 - start_time2))\n",
    "print(\"Waktu Milestone 5.3: %.2f detik\" % (end_time3 - start_time3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
